# ğŸŒ Web Sniffer

![Python](https://img.shields.io/badge/Python-3.11-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Beta-orange.svg)

`web-sniffer` est un framework Python pour scraper et agrÃ©ger automatiquement des donnÃ©es depuis plusieurs sources en ligne.  
Il prend en charge des moteurs de recherche et sites comme **Google**, **DuckDuckGo**, **Wikipedia**, et **JeuneAfrique**, avec **cache**, **retry**, et **multi-threading**.

---

## âš¡ FonctionnalitÃ©s principales

- Scraping multi-source et multi-pays.
- Cache JSON pour Ã©viter les requÃªtes rÃ©pÃ©tÃ©es.
- Gestion des erreurs, retry automatique et pause anti-429.
- ExÃ©cution parallÃ¨le via `ThreadPoolExecutor`.
- Rapport de performance dÃ©taillÃ© par source.
- Facile Ã  Ã©tendre avec de nouveaux scrapers.

---

## ğŸ—‚ Structure du projet

web-sniffer/
â”‚
â”œâ”€ core/
â”‚ â”œâ”€ cache.py # Interface et implÃ©mentation JSONCache
â”‚ â”œâ”€ config.py # Configuration globale (HTTP, cache, scraper runner)
â”‚ â”œâ”€ http_client.py # Fonctions HTTP avec retry et logging
â”‚ â”œâ”€ scraper_runner.py # ExÃ©cution parallÃ¨le des scrapers
â”‚ â””â”€ factory.py # Fabrique les instances de scrapers
â”‚
â”œâ”€ scrapers/
â”‚ â”œâ”€ base_scraper.py # Classe de base pour tous les scrapers
â”‚ â”œâ”€ google_search_scraper.py
â”‚ â”œâ”€ duckduckgo_scraper.py
â”‚ â”œâ”€ wikipedia_scraper.py
â”‚ â””â”€ jeune_afrique_scraper.py
â”‚
â”œâ”€ cache.json # Cache JSON gÃ©nÃ©rÃ© automatiquement
â”œâ”€ requirements.txt
â””â”€ README.md

yaml
Copy code

---

## âš™ï¸ Installation

### 1. Cloner le projet
```bash
git clone https://github.com/ton-utilisateur/web-sniffer.git
cd web-sniffer
2. CrÃ©er un environnement virtuel
bash
Copy code
python -m venv sniffer_venv
# macOS / Linux
source sniffer_venv/bin/activate
# Windows
sniffer_venv\Scripts\activate
3. Installer les dÃ©pendances
bash
Copy code
pip install -r requirements.txt
4. Configurer les variables d'environnement
CrÃ©er un fichier .env Ã  la racine du projet :

env
Copy code
GOOGLE_API_KEY=...
GOOGLE_CX_ID=...
DEFAULT_DATE_RANGE=w1
HTTP_TIMEOUT=10
HTTP_MAX_RETRIES=3
ANTI_429_MIN=4
ANTI_429_MAX=8
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...
CACHE_FILE=cache.json
USE_CACHE=true
MAX_WORKERS=5
LOG_LEVEL=INFO
ğŸš€ Utilisation
Lancer le scraping
bash
Copy code
python -m core.scraper_runner
Par dÃ©faut, le scraper utilise la liste de pays dÃ©finie dans scraper_runner.py :

python
Copy code
countries_list = ["Burkina Faso", "Senegal", "Cote d'Ivoire", "Mali", "Niger"]
RÃ©sultats
Les donnÃ©es sont stockÃ©es dans le cache JSON (cache.json).

La console affiche le nombre de rÃ©sultats par source et un rapport de performance.

Exemple de sortie JSON
json
Copy code
{
  "Burkina Faso": {
    "google": [
      {"title": "Ã‰conomie du Burkina Faso", "url": "...", "snippet": "..."}
    ],
    "duckduckgo": [
      {"title": "Burkina Faso Economy", "url": "...", "snippet": "..."}
    ],
    "wikipedia": [
      {"title": "Burkina Faso", "url": "...", "snippet": "..."}
    ],
    "jeuneafrique": [
      {"title": "Burkina Faso : ActualitÃ©s Ã©conomiques", "url": "...", "snippet": ""}
    ]
  }
}
ğŸ›  Ajouter un nouveau scraper
CrÃ©er une nouvelle classe dans scrapers/ hÃ©ritant de BaseScraper.

ImplÃ©menter les mÃ©thodes fetch_page(), parse(), et run().

Ajouter la source dans ScraperFactory :

python
Copy code
elif source == "nouvelle_source":
    return NouveauScraper(**kwargs)
Ajouter la source dans ScraperRunner.SCRAPER_SOURCES si nÃ©cessaire.

ğŸ”§ Configuration globale
Toutes les options sont centralisÃ©es dans core/config.py :

HTTP : timeout, headers, retry, pause anti-429

Cache : fichier, activation

Scraper Runner : nombre de threads (max_workers)

ğŸ“„ Licence
MIT License Â© 2025